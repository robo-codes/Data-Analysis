{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NpKX4tOyFlfY"
   },
   "source": [
    "# Coding One Hidden Layer\n",
    "\n",
    "## Method 1: Specify everything explicitly\n",
    "\n",
    "In this version, we will need to specify the input, parameters, and operations explicitly. Let's recall the architecture of the network we saw in the lectures:\n",
    "\n",
    "<img src=\"Lecture 7-5.jpg\" width=600 align=\"center\">\n",
    "\n",
    "Here we will reproduce the example calculation that we did by hand in **Lecture 7**. We first need to store the input values in separate variables. Here, we are considering only one *sample* and each sample has only *two* features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hjk7JKscFyxG"
   },
   "outputs": [],
   "source": [
    "x_1 = 2 \n",
    "x_2 = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to specify the parameters (the *weights* and *bias*) for each neuron in each layer. We talked a little bit about how a network *learns* but we will go into more detail about that next week. In order to do the first *forward pass* of the network we need to *initialize the parameters*, that is, pick starting values for the weights and biases. How you do this initialization can have an impact on how well the network learns. For our purposes right now, we just want to have some numbers so we can do the calculation. \n",
    "\n",
    "### Layer 1\n",
    "\n",
    "For layer 1, we have 2 neurons, and each of those neurons has 2 weights and 1 bias. We will use the notation *w = weight*, *b = bias*, *L = layer*, and *N = neuron* plus a number. So, `w1_N1_L1` is the first weight of the first neuron in layer 1, and so on.  And `b_N1_L1` is the bias for the first neuron of the first layer (we don't need `b1` and `b2` because each neuron only has a single bias), and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_N1_L1 = 0\n",
    "w2_N1_L1 = 1\n",
    "b_N1_L1 = 2\n",
    "\n",
    "w1_N2_L1 = 1\n",
    "w2_N2_L1 = 1\n",
    "b_N2_L1 = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now calculate the $z$ values for the 2 neurons in layer 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "U_v62HsZGCCQ",
    "outputId": "50939ad0-d883-402f-8a30-17590f783f2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_N1_L1 =  5\n",
      "z_N2_L1 =  6\n"
     ]
    }
   ],
   "source": [
    "z_N1_L1 = w1_N1_L1 * x_1 + w2_N1_L1 * x_2 + b_N1_L1\n",
    "z_N2_L1 = w1_N2_L1 * x_1 + w2_N2_L1 * x_2 + b_N2_L1\n",
    "\n",
    "print(\"z_N1_L1 = \", z_N1_L1)\n",
    "print(\"z_N2_L1 = \", z_N2_L1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to apply our *activation function* to these `z` values. We are going to use the *sigmoid* function, as we did in the lecture, so we need to first create a function for that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    a = 1 / (1 + np.exp(-z))\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then apply it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YCz7GXy_GJiR",
    "outputId": "3af3dbfa-104e-41da-99c6-8da2b35a2803"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output of N1_L1 is:  0.9933071490757153\n",
      "The output of N2_L1 is:  0.9975273768433653\n"
     ]
    }
   ],
   "source": [
    "a_N1_L1 = sigmoid(z_N1_L1)\n",
    "a_N2_L1 = sigmoid(z_N2_L1)\n",
    "\n",
    "print(\"The output of N1_L1 is: \", a_N1_L1)\n",
    "print(\"The output of N2_L1 is: \", a_N2_L1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output for each neuron in layer 1 is the same as what we did by hand in the lecture. (See the image at the beginning of this notebook.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 2\n",
    "\n",
    "For layer 2, our output layer, we have only 1 neuron. This neuron has 2 weights and 1 bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_N1_L2 = 2\n",
    "w2_N1_L2 = 0\n",
    "b_N1_L2 = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now calculate the $z$ value for this neuron, remembering that the inut values are no longer our training data but the output of the 2 neurons in layer 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "U_v62HsZGCCQ",
    "outputId": "50939ad0-d883-402f-8a30-17590f783f2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_N1_L2 =  2.9866142981514305\n"
     ]
    }
   ],
   "source": [
    "z_N1_L2 = w1_N1_L2 * a_N1_L1 + w2_N1_L2 * a_N2_L1 + b_N1_L2\n",
    "\n",
    "print(\"z_N1_L2 = \", z_N1_L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then apply the sigmoid activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YCz7GXy_GJiR",
    "outputId": "3af3dbfa-104e-41da-99c6-8da2b35a2803"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output of N1_L2 is:  0.9519657289209066\n"
     ]
    }
   ],
   "source": [
    "a_N1_L2 = sigmoid(z_N1_L2)\n",
    "\n",
    "print(\"The output of N1_L2 is: \", a_N1_L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this is the same result as we had in the lecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pv3R0HR9GYMP"
   },
   "source": [
    "## Method 2: Using NumPy arrays \n",
    "\n",
    "It should be obvious that Method 1 would be pretty much impossible for any realistic network, for example, one that takes in hundreds of feature values, has 5 hidden layers with 25 neurons each, etc. However, it is a good learning exercise to go through implementing our simpler network in this manner. \n",
    "\n",
    "So, for this method, we will repeat the example we did for Method 1 but use arrays and array operations instead. This will make the code simpler, less error prone, and faster. It will also allow us to put many samples through a forward pass of the network at once. Here, sample 1 = `[2, 3]` and sample 2 = `[4, 5]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HVWjAV5HHUT5"
   },
   "outputs": [],
   "source": [
    "X = np.array([[2, 3], [4, 5]])\n",
    "W_L1 = np.array([[0, 1], [1, 1]])\n",
    "b_L1 = np.array([2, 1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pBhhVHBEIEaY",
    "outputId": "bf0cc7c2-5522-4a99-9b9c-d80cb7a8aab4"
   },
   "outputs": [],
   "source": [
    "Z_L1 = np.dot(W_L1, X.T) + b_L1.reshape(-1, 1)\n",
    "Z_L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "f96HbBC7IIqZ",
    "outputId": "f834d0e8-9e40-42c8-c160-a4b7bbd8c155"
   },
   "outputs": [],
   "source": [
    "A_L1 = sigmoid(Z_L1)\n",
    "A_L1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see that the final output is an array with 4 numbers. The first row correspondes to the output of the neurons in layer 1 for when the input = `[2, 3]`. That is, when our input is `[2, 3]`, the first neuron of layer 1 will output `0.99330715` and the second neuron of layer 1 will output `0.99752738`. Similarly, when our input is `[4, 5]`, the first neuron of layer 1 will output `0.99908895` and the second neuron of layer 1 will output `0.9999546`.\n",
    "\n",
    "The second layer has one neuron with 2 weights and 1 bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_L2 = np.array([2, 0])\n",
    "b_L2 = np.array(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer now takes the output of layer 1 as its input, calculates a value for $z$ and applies the sigmoid activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_L2 = np.dot(W_L2, A_L1) + b_L2.reshape(-1, 1)\n",
    "Z_L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "f96HbBC7IIqZ",
    "outputId": "f834d0e8-9e40-42c8-c160-a4b7bbd8c155"
   },
   "outputs": [],
   "source": [
    "A_L2 = sigmoid(Z_L2)\n",
    "A_L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, when our input is `[3, 4]`, our neural network will output 0.95196573; or, you could say that for sample `[3, 4]`, our neural network will predict 0.95196573. And when our input is `[4, 5]`, our neural network will output 0.95249174; or, you could say that for sample `[4, 5]`, our neural network will predict 0.95249174. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AnSBwy05JVD1"
   },
   "source": [
    "## Method 3: Creating reusable objects\n",
    "\n",
    "As we did last week, we will now create our neural network using Python **classes**. Again, just focus on how this simplifies what we are trying to accomplish. \n",
    "\n",
    "In the code below, we are creating a recipe for a network with a single hidden layer. **Please note that we have not yet created an actual network.** This is similar to the difference between having a recipe for baking a loaf of bread and actually baking a loaf of bread. \n",
    "\n",
    "The **__init__** function will initialize all the weights and biases for all the neurons in each layer. The **feedforward** function then takes the input you provide to the network and does all of the array operations necessary for the network to produce output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8qra_ZLHIOsw",
    "outputId": "995bde7a-9bff-4f15-ac83-d2f1c0c2094c"
   },
   "outputs": [],
   "source": [
    "# Inspired by code written by Victor Zhou\n",
    "# License for reuse: https://github.com/vzhou842/neural-network-from-scratch/blob/master/LICENSE\n",
    "\n",
    "import numpy as np # included again here for completeness; normally this should only occur once in your notebook\n",
    "\n",
    "class OurNeuralNetwork:\n",
    "  '''\n",
    "  A neural network with:\n",
    "    - 2 inputs\n",
    "    - a hidden layer with 2 neurons (h1, h2)\n",
    "    - an output layer with 1 neuron (o1)\n",
    "  '''\n",
    "  def __init__(self, w_L1, w_L2, b_L1, b_L2):\n",
    "    # initialize the weights and biases for each layer\n",
    "    self.w1 = w_L1 \n",
    "    self.b1 = b_L1.reshape(-1, 1)\n",
    "    self.w2 = w_L2\n",
    "    self.b2 = b_L2.reshape(-1, 1)\n",
    "  \n",
    "  def activation(self, z):\n",
    "    a = 1 / (1 + np.exp(-z))\n",
    "    return a\n",
    "\n",
    "  def feedforward(self, input):\n",
    "    Z1 = np.dot(self.w1, input.T) + self.b1\n",
    "    A1 = self.activation(Z1)\n",
    "    Z2 = np.dot(self.w2, A1) + self.b2\n",
    "    A2 = self.activation(Z2)\n",
    "\n",
    "    out = A2\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some values for the weights and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OvlP5DEeJuiI"
   },
   "outputs": [],
   "source": [
    "w_L1 = np.array([[0, 1], [1, 1]]) \n",
    "b_L1 = np.array([2, 1])\n",
    "\n",
    "w_L2 = np.array([[2, 0]])\n",
    "b_L2 = np.array([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the recipe to create a single (specific) neuron. (That is, we are now using the recipe to bake a particular loaf of bread.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = OurNeuralNetwork(w_L1, w_L2, b_L1, b_L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line of code above creates a neuron with specific values for the weights and bias. It also already has a function that we can use to carry out the appropriate mathematical operations of our neuron to produce output whenever we supply values for the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[2, 3], [4, 5]])\n",
    "\n",
    "network.feedforward(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Single_neuron.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
