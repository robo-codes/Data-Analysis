{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NpKX4tOyFlfY"
   },
   "source": [
    "# Coding a Single Neuron\n",
    "## Method 1: Specify everything explicitly\n",
    "\n",
    "In this version, we will need to specify the input, parameters, and operations explicitly. While this is fine for a small example, it should start to feel impractical as the number of inputs and parameters increases, even for a single neuron. But, it is a good place to start to learn exactly what a neuron does. To start, let's recall the details of a single neuron:\n",
    "\n",
    "<img src=\"single_neuron.jpeg\" width=600 align=\"center\">\n",
    "\n",
    "We first need to store the input values in separate variables. Here, we are considering only one *sample* (think of this like one row of data from a spreadsheet) and each sample has only *two* features (think of this like two columns from a spreadsheet). In a more realistic setting, we could have thousands or millions of samples to deal with, and each one could have hundreds of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hjk7JKscFyxG"
   },
   "outputs": [],
   "source": [
    "x_1 = 3 \n",
    "x_2 = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to specify the parameters: the *weights* and *bias*. We need one weight for each input feature. Later we will see how the neuron *learns* the best values for the weights and bias. At this point we are doing what is called *weight initialization*: picking starting values for the weights and bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_1 = 1\n",
    "w_2 = 0\n",
    "\n",
    "b = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to specify the first *mathematical operation* that the neuron perfoms: a linear transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "U_v62HsZGCCQ",
    "outputId": "50939ad0-d883-402f-8a30-17590f783f2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = w_1 * x_1 + w_2 * x_2 + b\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we perform the final mathematical operation, which is applying the *activation function* to the output of the previous step. Here, we are using the *ReLU* function, or *Rectified Linear Unit*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YCz7GXy_GJiR",
    "outputId": "3af3dbfa-104e-41da-99c6-8da2b35a2803"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = max(0, z)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above is the **$\\hat y$** in the image above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pv3R0HR9GYMP"
   },
   "source": [
    "## Method 2: Using NumPy arrays \n",
    "\n",
    "There are at least two problems with **Method 1**. The first is that as the number of inputs increases, for example, imagine samples with 1000 features, this approach is impractical as you would need to specify 1000 variables for the input data and 1000 variables for the weights which ends up being a lot of unnecessary typing (if you don't believe it, try doing it!). The second problem is that we are not taking advantage of packages like NumPy that optimize for these mathematical operations. \n",
    "\n",
    "So, for this method, we will repeat the example we did for Method 1 but use arrays and array operations instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zleeaTw9GSGR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([3, 4])\n",
    "w = np.array([1, 0])\n",
    "\n",
    "# the next statement is similar to \"b = 3\" \n",
    "# this may look odd but will help later when we have more than one neuron and thus more than one bias\n",
    "b = np.array(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take advantage of NumPy's optimized method for doing mathematical operations with arrays. If we consider the weights and the features to be vectors: \n",
    "\n",
    "$$\\boldsymbol{w} = [ w_1, w_2] \\\\\n",
    "\\boldsymbol{x} = [x_1, x_2] $$\n",
    "\n",
    "Then all we need to do is calculate what is called the *dot product* of these two vectors:\n",
    "\n",
    "$$\\rm{np.dot}(\\boldsymbol{w}, \\boldsymbol{x}) = w_1 x_1 + w_2 x_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "A2B4_JqTHKGb",
    "outputId": "5335963b-8792-4003-ed52-9df39ccd0534"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.dot(w, x.T) + b\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zrqSyLGXHQKJ",
    "outputId": "ac4be3b0-7876-441f-e9b3-6c8de7e11aa0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = max(0, z)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we get the same result as we did with Method 1. However, we could now use Pandas to read in a .csv file and store that in a NumPy array (our $x$ variable in the above code) and have a much quicker method for getting the input data into our code. Although there will be no speed difference for examples like ours with only a few numbers, we will notice a difference when we have thousands of samples and thousands of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hd1hfDkXH2_v"
   },
   "source": [
    "### Extending Method 2: Multiple samples\n",
    "\n",
    "Our examples so far have only consider one sample, which would never be the case for a real application. Using arrays allows us to extend the operation of a single neuron to multiple samples. The code below repeats what we did above except for two samples.\n",
    "\n",
    "So, in the example below, the input [3, 4] is passed through the single neuron to produce a single output and then the input [2, 5] is passed through the single neuron to produce another single output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HVWjAV5HHUT5"
   },
   "outputs": [],
   "source": [
    "x = np.array([[3, 4], [2, 5]])\n",
    "w = np.array([1, 0])\n",
    "b = np.array(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pBhhVHBEIEaY",
    "outputId": "bf0cc7c2-5522-4a99-9b9c-d80cb7a8aab4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.dot(w, x.T) + b\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "f96HbBC7IIqZ",
    "outputId": "f834d0e8-9e40-42c8-c160-a4b7bbd8c155"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.maximum(0, z)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see that the final output is an array with two numbers. Here the output 6 is associated with the input [3, 4] and the output 5 is associated with the input [2, 5]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AnSBwy05JVD1"
   },
   "source": [
    "## Method 3: Creating reusable objects\n",
    "\n",
    "While Method 2 is an improvement over Method 1, we can do even better by taking advantage of the fact that Python is an object-oriented programming language. For this course, you do not need to worry about being able write your own Python **classes**. What I would like you to focus on is how this simplifies what we are trying to accomplish: create a network out of a bunch of neurons where each neuron has the same fundamental structure and operation. \n",
    "\n",
    "To do this, we can use the **class** keyword instead of having to write similar code over and over again for each neuron. We can think of a class as a recipe for a particular type of object. And an object is a collection of *variables* and *functions* (also called *methods*) that act on those variables.\n",
    "\n",
    "In the code below, we are creating a recipe for a single neuron. **Please note that we have not yet created an actual neuron.** This is similar to the difference between having a recipe for baking a loaf of bread and actually baking a loaf of bread. \n",
    "\n",
    "We know that any neuron needs to have weights and a bias. That is what the **__init__** function takes care of. We also know that our neuron has to perform certain mathematical operations on the input, weights, and bias and provide an output. That is what the **feedforward** function takes care of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8qra_ZLHIOsw",
    "outputId": "995bde7a-9bff-4f15-ac83-d2f1c0c2094c"
   },
   "outputs": [],
   "source": [
    "# This code is based on code written by Victor Zhou\n",
    "# License for reuse: https://github.com/vzhou842/neural-network-from-scratch/blob/master/LICENSE\n",
    "\n",
    "class Neuron:\n",
    "  # sets the value of the weights and the bias\n",
    "  def __init__(self, weights, bias):\n",
    "    self.weights = weights\n",
    "    self.bias = bias\n",
    "\n",
    "  # defines the mathematical operations to be done on the inputs, weights, and bias to produce the output\n",
    "  def feedforward(self, inputs):\n",
    "    z = np.dot(self.weights, inputs.T) + self.bias\n",
    "    a = np.maximum(0, z)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some values for the weights and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OvlP5DEeJuiI"
   },
   "outputs": [],
   "source": [
    "w = np.array([1, 0]) \n",
    "b = np.array(3)                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the recipe to create a single (specific) neuron. (That is, we are now using the recipe to bake a particular loaf of bread.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_neuron = Neuron(w, b)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line of code above creates a neuron with specific values for the weights and bias. It also already has a function that we can use to carry out the appropriate mathematical operations of our neuron to produce output whenever we supply values for the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[3, 4], [2, 5]])      \n",
    "\n",
    "my_neuron.feedforward(x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Single_neuron.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
