{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "TR2CPIZ9tNDE",
   "metadata": {
    "id": "TR2CPIZ9tNDE"
   },
   "source": [
    "# Lab 3 Part 1: Convolutions in Keras\n",
    "The code below will allow you to play with a single convolutional layer in Keras. Take a look at the documentation for the Conv2D layer, which is also where the original code came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wO83RrBkuB0F",
   "metadata": {
    "id": "wO83RrBkuB0F"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "VyXPGKnPuKWc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VyXPGKnPuKWc",
    "outputId": "808dffa9-9624-4e71-b069-4571ace42d91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 26, 26, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = (4, 28, 28, 3)\n",
    "\n",
    "x = tf.random.normal(input_shape)\n",
    "\n",
    "y = Conv2D(filters=2,\n",
    "           kernel_size=(3, 3),\n",
    "           strides=1,\n",
    "           padding='valid',\n",
    "           input_shape=(None, 28, 28, 3))(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K1ciM03Ct9A2",
   "metadata": {
    "id": "K1ciM03Ct9A2"
   },
   "source": [
    "Here is a brief explanation of the above code:\n",
    "\n",
    "<img src=\"Convolution_notebook-2.jpg\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JMlk6aIouaIF",
   "metadata": {
    "id": "JMlk6aIouaIF"
   },
   "source": [
    "# Exercises\n",
    "In the code above, make changes to:\n",
    "\n",
    "- input  ‚Ñé\n",
    "- input  ùë§\n",
    "- input  ùëõùëê\n",
    "- number of filters\n",
    "- kernel size (same as filter size)\n",
    "\n",
    "For each change, calculate the dimensions of the output (y.shape) by hand, including drawing a diagram (as shown below).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U29TU5wyu8D7",
   "metadata": {
    "id": "U29TU5wyu8D7"
   },
   "source": [
    "<img src=\"Convolution_notebook-1.jpg\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac958f2",
   "metadata": {
    "id": "dac958f2"
   },
   "source": [
    "## MNIST Revisited\n",
    "\n",
    "Let's now revisit our MNIST. Knowing that the data contains 2-dimensional images of handwritten digits, we should be able to apply what we've learned about convolutions. Thus, in this section, we will create a convolutional neural network (CNN or convnet) for this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e2399a",
   "metadata": {
    "id": "12e2399a"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07d1bca",
   "metadata": {
    "id": "a07d1bca"
   },
   "outputs": [],
   "source": [
    "train_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd87f235",
   "metadata": {
    "id": "fd87f235"
   },
   "outputs": [],
   "source": [
    "train_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0933573f",
   "metadata": {
    "id": "0933573f"
   },
   "source": [
    "This time we are going to use a **validation set** to monitor our training progress. We can also use this validation set for *hyperparameter tuning*. Remember, using the validation set allows us to keep the *test set* to gauge how well our final model should do in the real world; that is, the final model only sees the test data once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c7956",
   "metadata": {
    "id": "c20c7956"
   },
   "outputs": [],
   "source": [
    "# Use the first 10,000 samples of our training data as our validation set\n",
    "val_data = train_data[:10000]\n",
    "val_labels = train_labels[:10000]\n",
    "\n",
    "# Use the remainder of the original training data for actual training\n",
    "partial_train_data = train_data[10000:]\n",
    "partial_train_labels = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd053ac2",
   "metadata": {
    "id": "cd053ac2"
   },
   "outputs": [],
   "source": [
    "# Scale the pixel values so they lie in the range of 0-1\n",
    "partial_train_data = partial_train_data / 255.\n",
    "val_data = val_data / 255.\n",
    "\n",
    "test_data = test_data /255."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7205cd6d",
   "metadata": {
    "id": "7205cd6d"
   },
   "source": [
    "Note that our data currently has 3 dimensions: `(samples, height, width)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d383bcb",
   "metadata": {
    "id": "9d383bcb"
   },
   "outputs": [],
   "source": [
    "print(partial_train_data.shape)\n",
    "print(val_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9a56ac",
   "metadata": {
    "id": "9d9a56ac"
   },
   "outputs": [],
   "source": [
    "print(partial_train_labels.shape)\n",
    "print(val_labels.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8587a274",
   "metadata": {
    "id": "8587a274"
   },
   "source": [
    "Our convolutional neural network will expect 4-dimensional data: `(batch_size, height, width, channels)`. Note that depending on how you decide to update the parameters of the network, `batch_size` could equal the number of `samples` (as in *batch gradient descent*), or it could equal a single sample (as in *stochastic gradient descent*, or it can equal the batch size (as in *mini-batch gradient descent*).\n",
    "\n",
    "We can use a NumPy function to add this dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbea7c1",
   "metadata": {
    "id": "acbea7c1"
   },
   "outputs": [],
   "source": [
    "partial_train_data = np.expand_dims(partial_train_data, axis=3)\n",
    "val_data = np.expand_dims(val_data, axis=3)\n",
    "test_data = np.expand_dims(test_data, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536f781e",
   "metadata": {
    "id": "536f781e"
   },
   "outputs": [],
   "source": [
    "print(partial_train_data.shape)\n",
    "print(val_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638d8eeb",
   "metadata": {
    "id": "638d8eeb"
   },
   "source": [
    "Note how a fourth dimension was added to our data. This dimension corresponds to the number of channels in our input data. Here it is 1, since the images are all greyscale. It would be 3 if the images were RGB. Also note, that the convention here is *channels last*, as opposed to *channels first*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d9b7e4",
   "metadata": {
    "id": "76d9b7e4"
   },
   "source": [
    "As in Lab 1, we need to convert our label data to the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1846a1",
   "metadata": {
    "id": "6f1846a1"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "partial_train_labels = to_categorical(partial_train_labels)\n",
    "val_labels = to_categorical(val_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c4ca3",
   "metadata": {
    "id": "c78c4ca3"
   },
   "outputs": [],
   "source": [
    "print(partial_train_labels.shape)\n",
    "print(val_labels.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a79f2",
   "metadata": {
    "id": "da4a79f2"
   },
   "source": [
    "We will now import the necessary modules for building our convolutional neural network. Since we are using Keras's sequential API we need to import the `Sequential` module. The remaining 3 imports will help us build the layers of our CNN. `Conv2D` creates the convolutional layers we have been discussing in the lectures. `Flatten` is used to create a 1 dimensional vector so we can feed the output of our convolutional layers to the fully-connected layers. We used NumPy's `reshape` function to do this flattening in Lab 1. And the `Dense` layer is the same as what we used in Lab 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8ac061",
   "metadata": {
    "id": "9f8ac061"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b604145d",
   "metadata": {
    "id": "b604145d"
   },
   "source": [
    "We are going to use a slightly different approach to building our network than we did in Lab 1. Here we will directly add a *list of layers* to the `Sequential()` object. That is, we put all our layers inside square brackets `[...]` and put this inside the `Sequential( [...] )` object to create our model. In Lab 1 we used the `.add()` method to add individual layers to our `Sequential()` object that we initialized without any layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79abd458",
   "metadata": {
    "id": "79abd458"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(filters=32,\n",
    "           kernel_size=(3, 3),\n",
    "           strides=1,\n",
    "           padding='same',\n",
    "           activation='relu',\n",
    "           input_shape=(28, 28, 1)),\n",
    "    Conv2D(filters=32,\n",
    "           kernel_size=(3, 3),\n",
    "           strides=2,\n",
    "           padding='valid',\n",
    "           activation='relu'),\n",
    "    Conv2D(filters=64,\n",
    "           kernel_size=(3, 3),\n",
    "           strides=1,\n",
    "           padding='same',\n",
    "          activation='relu'),\n",
    "    Conv2D(filters=64,\n",
    "           kernel_size=(3, 3),\n",
    "           strides=1,\n",
    "           padding='valid',\n",
    "           activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ca17b7",
   "metadata": {
    "id": "79ca17b7"
   },
   "source": [
    "It is often helpful to see the tensor shapes and number of parameters per layer. We can get this information by using the `.summary()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd3649d",
   "metadata": {
    "id": "cfd3649d"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59f6c82",
   "metadata": {
    "id": "a59f6c82"
   },
   "source": [
    "We are still tackling the same type of problem (multi-class classification) so the same loss and metrics will work for us here. The optimizer `rmsprop` is the same as we used before and can be taken as the default method (or recipe) to try out for updating the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192f7772",
   "metadata": {
    "id": "192f7772"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bce024b",
   "metadata": {
    "id": "3bce024b"
   },
   "source": [
    "We now fit our model to the remaining training data (the original training data minus the validation data). You will now see that *loss* and *accuracy* get updated for each batch of images (here set to 256) but the *validation loss* and *validation accuracy* get updated after each *epoch*. Note that the *validation data* is not being used to train the model. Each batch of the training data is used to update the parameters and then, once we have gone through all of the samples in our training data (that is, all the samples in `partial_train_data`) the model is used to make predictions for the validation set. From those predictions the validation loss and accuracy are calculated.\n",
    "\n",
    "Each epoch of training should take 30-50s to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f314531",
   "metadata": {
    "id": "8f314531"
   },
   "outputs": [],
   "source": [
    "history = model.fit(partial_train_data,\n",
    "                    partial_train_labels,\n",
    "                    epochs=10,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val_data, val_labels),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a38a57",
   "metadata": {
    "id": "37a38a57"
   },
   "source": [
    "The values for the training loss and accuracy, as well as the validation loss and accuracy, are stored in the `history` variable. You can see the structure of the dictionary that stores this information as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50091fd0",
   "metadata": {
    "id": "50091fd0"
   },
   "outputs": [],
   "source": [
    "history.history['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d77157e",
   "metadata": {
    "id": "5d77157e"
   },
   "source": [
    "We will now use this information to visualize the progress our network makes on the loss and accuracy as the number of epochs increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4997ef2b",
   "metadata": {
    "id": "4997ef2b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # needed to create our plot\n",
    "\n",
    "history_dict = history.history # the dictionary that has the information on loss and accuracy per epoch\n",
    "\n",
    "loss_values = history_dict['loss']   # training loss\n",
    "val_loss_values = history_dict['val_loss'] # validation loss\n",
    "\n",
    "epochs = range(1, len(loss_values)+1)  #creates list of integers to match the number of epochs of training\n",
    "\n",
    "# code to plot the results\n",
    "plt.plot(epochs, loss_values, 'b', label=\"Training Loss\")\n",
    "plt.plot(epochs, val_loss_values, 'r', label=\"Validation Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(epochs)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b78182",
   "metadata": {
    "id": "f5b78182"
   },
   "outputs": [],
   "source": [
    "# As above, but this time we want to visualize the training and validation accuracy\n",
    "acc_values = history_dict['accuracy']\n",
    "val_acc_values = history_dict['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'b', label=\"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc_values, 'r', label=\"Validation Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(epochs)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f89c1b-c3e4-47cc-b083-3dbe056c4926",
   "metadata": {
    "id": "16f89c1b-c3e4-47cc-b083-3dbe056c4926"
   },
   "source": [
    "## Exercise: Change the layers\n",
    "\n",
    "Play around with the **number of filters** and the **filter size** in our model. Note the change in:\n",
    "- number of parameters in the model\n",
    "- training and validation losses and accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b79813e-127a-48fc-aa92-c4b4453a6da4",
   "metadata": {
    "id": "9b79813e-127a-48fc-aa92-c4b4453a6da4",
    "tags": []
   },
   "source": [
    "### Exercise: Early Stopping\n",
    "\n",
    "When you have a final model, train it until the validation loss stops decreasing. At this point, the model will have stopped learning and will start to memorize the training data. The model may be starting to overfit. Note the number of epochs at which this happens.  One way to avoid this overfitting is called *early stopping*.  \n",
    "\n",
    "Try implementing early stopping for our model:\n",
    "- use the validation loss plot to determine which epoch corresponds to when the model stops learning\n",
    "    - if it so happens that the validation loss continues going down for all 10 epochs, then increase the number of epochs in the original code to 20\n",
    "- use the complete training set (no validation set)\n",
    "- scale this training set\n",
    "- expand its dimensions to 4\n",
    "- use the same model, and same optimizer, loss and metrics\n",
    "- fit the model to the complete training set (no validation set)\n",
    "- evaluate the trained model on the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9226968",
   "metadata": {
    "id": "c9226968"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "191cecc7-466c-4fee-80f5-31c8f80c9f72",
   "metadata": {
    "id": "191cecc7-466c-4fee-80f5-31c8f80c9f72"
   },
   "source": [
    "### Exercise: Early Stopping with Callbacks\n",
    "\n",
    "Now try to implement early stopping using the Keras [callback](https://keras.io/api/callbacks/early_stopping/) functionality. In this case, you will need to use the validation data, because you want the early stopping to occur as a result of Keras monitoring the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae1ec6-d28f-434f-a503-04d4d18bd892",
   "metadata": {
    "id": "b7ae1ec6-d28f-434f-a503-04d4d18bd892"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
