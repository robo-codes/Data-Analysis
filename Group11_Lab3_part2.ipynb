{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OXcrFmMG4YH"
   },
   "source": [
    "# Lab 3 Part 2 - Task 1: Parameters in CNN (5 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRd3WOPoG0T4"
   },
   "source": [
    "- For the model we have created in **Lab 3 Part 1 Exercise**: Early Stopping with Callbacks, calculate the number of parameters by hand for each layer and compare to the output of model.summary() and print the model summary.\n",
    "- Then print the model summary of **Exercise 7 in Lab 1**\n",
    "- Now compare the Model you created in **Exercise 7 in Lab 1**,\n",
    "  - Compare the Parameters of the models\n",
    "\n",
    "  - Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1oFUB5Cd4vvL",
    "outputId": "d15140e2-a5af-490c-a2a2-41f8f1e547ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Dropout, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.datasets import mnist, cifar10\n",
    "import numpy as np\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GkOCvDy_FttX"
   },
   "outputs": [],
   "source": [
    "# Set the random seed to get same results everytime.\n",
    "tf.random.set_seed(101)\n",
    "np.random.seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8Ss02TvtFwLs"
   },
   "outputs": [],
   "source": [
    "# Use the first 10,000 samples of our training data as our validation set\n",
    "val_data = train_data[:10000]\n",
    "val_labels = train_labels[:10000]\n",
    "\n",
    "# Use the remainder of the original training data for actual training\n",
    "partial_train_data = train_data[10000:]\n",
    "partial_train_labels = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GOVF5MGEGNcI"
   },
   "outputs": [],
   "source": [
    "# Scale the pixel values so they lie in the range of 0-1\n",
    "partial_train_data = partial_train_data / 255.\n",
    "val_data = val_data / 255.\n",
    "test_data = test_data /255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XvDN4WmJGOmF"
   },
   "outputs": [],
   "source": [
    "# Expanding the dimensions of partial_train_data, val_data and test_data to make it 4D.\n",
    "partial_train_data = np.expand_dims(partial_train_data, axis=3)\n",
    "val_data = np.expand_dims(val_data, axis=3)\n",
    "test_data = np.expand_dims(test_data, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bW9Yy7POGPm7"
   },
   "outputs": [],
   "source": [
    "# Converting the labels into one hot encoded format\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "partial_train_labels = to_categorical(partial_train_labels)\n",
    "val_labels = to_categorical(val_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FbFR0A2GGQz4",
    "outputId": "60a3a866-e3d6-4cfa-b2c9-237ca0e7fdad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "196/196 [==============================] - 4s 16ms/step - loss: 0.3156 - accuracy: 0.9081 - val_loss: 0.0775 - val_accuracy: 0.9778\n",
      "Epoch 2/15\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 0.0568 - accuracy: 0.9825 - val_loss: 0.0701 - val_accuracy: 0.9798\n",
      "Epoch 3/15\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 0.0334 - accuracy: 0.9898 - val_loss: 0.1205 - val_accuracy: 0.9705\n",
      "Epoch 4/15\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 0.0216 - accuracy: 0.9934 - val_loss: 0.0601 - val_accuracy: 0.9849\n",
      "Epoch 5/15\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 0.0151 - accuracy: 0.9955 - val_loss: 0.0484 - val_accuracy: 0.9881\n",
      "Epoch 6/15\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 0.0112 - accuracy: 0.9966 - val_loss: 0.0607 - val_accuracy: 0.9862\n",
      "Epoch 7/15\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 0.0088 - accuracy: 0.9973 - val_loss: 0.0535 - val_accuracy: 0.9894\n",
      "Epoch 8/15\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 0.0066 - accuracy: 0.9979 - val_loss: 0.0570 - val_accuracy: 0.9888\n",
      "Epoch 9/15\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 0.0051 - accuracy: 0.9986 - val_loss: 0.0537 - val_accuracy: 0.9899\n",
      "Epoch 10/15\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 0.0036 - accuracy: 0.9991 - val_loss: 0.0642 - val_accuracy: 0.9893\n",
      "Epoch 11/15\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.0699 - val_accuracy: 0.9895\n",
      "Epoch 12/15\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.0659 - val_accuracy: 0.9904\n",
      "Epoch 13/15\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.0703 - val_accuracy: 0.9900\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 13, 13, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 13, 13, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 11, 11, 128)       73856     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 15488)             0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               1982592   \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2085802 (7.96 MB)\n",
      "Trainable params: 2085802 (7.96 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0522 - accuracy: 0.9906\n",
      "Test Accuracy: 0.9905999898910522\n"
     ]
    }
   ],
   "source": [
    "# Define a Sequential model for the CNN architecture.\n",
    "model = Sequential([\n",
    "    # Convolutional Layer 1\n",
    "    Conv2D(filters=32,\n",
    "           kernel_size=(3, 3),\n",
    "           strides=1,\n",
    "           padding='same',\n",
    "           activation='relu',\n",
    "           input_shape=(28, 28, 1)),\n",
    "\n",
    "    # Convolutional Layer 2\n",
    "    Conv2D(filters=32,\n",
    "           kernel_size=(3, 3),\n",
    "           strides=2,\n",
    "           padding='valid',\n",
    "           activation='relu'),\n",
    "\n",
    "    # Convolutional Layer 3\n",
    "    Conv2D(filters=64,\n",
    "           kernel_size=(3, 3),\n",
    "           strides=1,\n",
    "           padding='same',\n",
    "           activation='relu'),\n",
    "\n",
    "    # Convolutional Layer 4\n",
    "    Conv2D(filters=128,\n",
    "           kernel_size=(3, 3),\n",
    "           strides=1,\n",
    "           padding='valid',\n",
    "           activation='relu'),\n",
    "\n",
    "    # Flatten Layer to convert the 2D feature maps to a 1D vector\n",
    "    Flatten(),\n",
    "\n",
    "    # Fully Connected Layer 1\n",
    "    Dense(128, activation='relu'),\n",
    "\n",
    "    # Output Layer with 10 units and softmax activation for classification\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model, specifying the optimizer, loss function, and metrics.\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Define an early stopping callback to monitor the training process.\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)\n",
    "\n",
    "# Train the model using the provided data and labels.\n",
    "model_history = model.fit(partial_train_data,\n",
    "                          partial_train_labels,\n",
    "                          epochs=15,\n",
    "                          batch_size=256,\n",
    "                          callbacks=[callback],\n",
    "                          validation_data=(val_data, val_labels),\n",
    "                          verbose=1)\n",
    "\n",
    "# Display a summary of the model's architecture.\n",
    "model.summary()\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Yhey_YS3GSel"
   },
   "outputs": [],
   "source": [
    "# Prepare image data for a feedforward neural network (FNN):\n",
    "\n",
    "train_images_fnn = partial_train_data.astype('float32') / 255\n",
    "train_images_fnn = partial_train_data.reshape((50000, 28 * 28))\n",
    "\n",
    "test_images_fnn = val_data.astype('float32') / 255\n",
    "test_images_fnn = val_data.reshape((10000, 28 * 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IKEQYOD2GU5B",
    "outputId": "22bf3951-a214-4340-af24-fe76d70a184e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 500)               392500    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               50100     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 443610 (1.69 MB)\n",
      "Trainable params: 443610 (1.69 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1471/1471 [==============================] - 8s 4ms/step - loss: 0.2183 - accuracy: 0.9335\n",
      "Epoch 2/5\n",
      "1471/1471 [==============================] - 5s 3ms/step - loss: 0.0902 - accuracy: 0.9729\n",
      "Epoch 3/5\n",
      "1471/1471 [==============================] - 6s 4ms/step - loss: 0.0629 - accuracy: 0.9796\n",
      "Epoch 4/5\n",
      "1471/1471 [==============================] - 5s 4ms/step - loss: 0.0448 - accuracy: 0.9857\n",
      "Epoch 5/5\n",
      "1471/1471 [==============================] - 5s 4ms/step - loss: 0.0359 - accuracy: 0.9886\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0827 - accuracy: 0.9773\n",
      "Test Accuracy: 0.9772999882698059\n"
     ]
    }
   ],
   "source": [
    "# Create a Sequential FNN model\n",
    "sequential_7 = Sequential()\n",
    "\n",
    "# Define the FNN architecture\n",
    "sequential_7.add(Dense(500, activation='relu', input_shape=(28 * 28,)))\n",
    "sequential_7.add(Dense(100, activation='tanh', kernel_initializer='glorot_uniform'))\n",
    "sequential_7.add(Dropout(0.25))\n",
    "sequential_7.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Display model summary\n",
    "sequential_7.summary()\n",
    "\n",
    "# Compile the FNN\n",
    "sequential_7.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the FNN\n",
    "sequential_7.fit(train_images_fnn, partial_train_labels, epochs=5, batch_size=34, verbose=1)\n",
    "\n",
    "# Evaluate the FNN\n",
    "test_loss, test_acc = sequential_7.evaluate(test_images_fnn, val_labels)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXsFN7qTH2y7"
   },
   "source": [
    "#**Comparison between above two models.**\n",
    "\n",
    "**1. FNN (Feed-Forward Neural Network):**\n",
    ">- ***Model Architecture:*** The feed-forward neural network (FNN) consists of two dense (fully connected) layers with 500 and 100 units, a dropout layer, and a final dense layer with 10 output units for classification.\n",
    "\n",
    ">- ***Total Parameters:*** The FNN has a total of 443,610 trainable parameters (approximately 1.69 MB).\n",
    "\n",
    ">- ***Training:*** The FNN was trained for 5 epochs with a batch size of 1471. The training accuracy increased from around 93.4% in the first epoch to 98.9% in the fifth epoch.\n",
    "\n",
    ">- ***Validation:*** The validation accuracy reached 97.73% after training.\n",
    "\n",
    ">- ***Test Accuracy:*** The test accuracy achieved was 97.73%.\n",
    "\n",
    "**2. CNN (Convolutional Neural Network):**\n",
    ">- ***Model Architecture:*** The CNN consists of four convolutional layers with various filter sizes, max-pooling layers, dropout layers, a flatten layer, a dense layer with 128 units, and a final dense layer with 10 output units for classification.\n",
    "\n",
    ">- ***Total Parameters:*** The CNN has a total of 2,085,802 trainable parameters (approximately 7.96 MB).\n",
    "\n",
    ">- ***Training:*** The CNN was trained for 15 epochs with a batch size of 196. The training accuracy increased from 90.81% in the first epoch to 99.95% in later epochs.\n",
    "\n",
    ">- ***Validation:*** The validation accuracy reached 99.04% after training.\n",
    "\n",
    ">- ***Test Accuracy:*** The test accuracy achieved was 99.06%.\n",
    "\n",
    "\n",
    "1. The CNN architecture achieved higher accuracy on both the training and validation sets compared to the FNN.\n",
    "\n",
    "2. The CNN's ability to capture spatial features in the data contributed to better performance.\n",
    "\n",
    "3. The CNN also has more parameters due to the convolutional layers, which can handle complex patterns in the data.\n",
    "\n",
    "4. The FNN, while still achieving a high accuracy, had fewer parameters and might be more suitable for simpler datasets.\n",
    "\n",
    "\n",
    "In Summary, the CNN performs better in terms of accuracy which comes with higher computational costs due to the number of parameters being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WftJ0QbNHKmI"
   },
   "source": [
    "# Lab 3 Part 2 - Task 2: CIFAR-10 Challenge (10 Marks)\n",
    "\n",
    "In this lab you will experiment with whatever ConvNet architecture/design you'd like on [CIFAR-10 image dataset](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "\n",
    "## Exercise  1: Creating the network\n",
    "\n",
    "**Goal:** After training, your model should achieve **at least 80%** accuracy on a **validation** set within 20 epochs. (Or as close as possible as long as there is demonstrated effort to achieve this goal.)\n",
    "\n",
    "**Data split** The training set should consist of 40000 images, the validation set should consist of 10000 images, and the test set should consist of the remaining 10000 images. **Please use the Keras `load_data()` function to import the data set.**\n",
    "\n",
    "\n",
    "### Some things you can try:\n",
    "- Different number/type of layers\n",
    "- Different filter sizes\n",
    "- Adjust the number of filters used in any given layer\n",
    "- Try various pooling strategies\n",
    "- Consider using batch normalization\n",
    "- Check if adding regularization helps\n",
    "- Consider alternative optimizers\n",
    "- Try different activation functions\n",
    "\n",
    "\n",
    "### Tips for training\n",
    "When building/tuning your model, keep in mind the following points:\n",
    "\n",
    "- This is experimental, so be driven by results achieved on the validation set as opposed to what you have heard/read works well or doesn't\n",
    "- If the hyperparameters are working well, you should see improvement in the loss/accuracy within approximately one epoch\n",
    "- For hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all\n",
    "- Once you have found some sets of hyperparameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
    "- Prefer random search to grid search for hyperparameters\n",
    "- You should use the validation set for hyperparameter search and for evaluating different architectures\n",
    "- The test set should only be used at the very end to evaluate your final model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_KaA347LHO1t",
    "outputId": "f740376c-0e18-40eb-bd16-e4542faba300"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 2s 0us/step\n",
      "(40000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = cifar10.load_data()\n",
    "\n",
    "# Use the first 10,000 samples of our training data as our validation set\n",
    "val_data_cifar = train_data[:10000]\n",
    "val_labels_cifar = train_labels[:10000]\n",
    "\n",
    "# Use the remainder of the original training data for actual training\n",
    "train_data_cifar = train_data[10000:]\n",
    "train_labels_cifar = train_labels[10000:]\n",
    "\n",
    "# Scale the pixel values so they lie in the range of 0-1\n",
    "train_data_cifar = train_data_cifar/ 255.\n",
    "val_data_cifar = val_data_cifar / 255.\n",
    "test_data_cifar = test_data /255.\n",
    "\n",
    "print(train_data_cifar.shape)\n",
    "print(val_data_cifar.shape)\n",
    "print(test_data_cifar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RBOd2TgXGn7p",
    "outputId": "e9d33f31-5415-4793-8e2a-fdb323116ff7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 10)\n",
      "(10000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# convert the labels to categorical data\n",
    "\n",
    "train_labels_cifar = to_categorical(train_labels_cifar)\n",
    "val_labels_cifar = to_categorical(val_labels_cifar)\n",
    "test_labels_cifar = to_categorical(test_labels)\n",
    "\n",
    "print(train_labels_cifar.shape)\n",
    "print(val_labels_cifar.shape)\n",
    "print(test_labels_cifar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4CFSkqxyXQ8b",
    "outputId": "15d85f8a-8f15-48be-cb70-685fef02d589"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "267/267 [==============================] - 30s 94ms/step - loss: 2.0261 - accuracy: 0.2677 - val_loss: 2.3111 - val_accuracy: 0.2008\n",
      "Epoch 2/20\n",
      "267/267 [==============================] - 23s 85ms/step - loss: 1.4998 - accuracy: 0.4518 - val_loss: 1.5979 - val_accuracy: 0.4387\n",
      "Epoch 3/20\n",
      "267/267 [==============================] - 23s 85ms/step - loss: 1.2273 - accuracy: 0.5631 - val_loss: 1.0139 - val_accuracy: 0.6507\n",
      "Epoch 4/20\n",
      "267/267 [==============================] - 22s 84ms/step - loss: 1.0645 - accuracy: 0.6282 - val_loss: 0.9147 - val_accuracy: 0.6771\n",
      "Epoch 5/20\n",
      "267/267 [==============================] - 23s 84ms/step - loss: 0.9565 - accuracy: 0.6649 - val_loss: 0.8731 - val_accuracy: 0.6949\n",
      "Epoch 6/20\n",
      "267/267 [==============================] - 22s 84ms/step - loss: 0.8781 - accuracy: 0.6967 - val_loss: 0.7754 - val_accuracy: 0.7289\n",
      "Epoch 7/20\n",
      "267/267 [==============================] - 22s 84ms/step - loss: 0.8157 - accuracy: 0.7186 - val_loss: 0.7436 - val_accuracy: 0.7438\n",
      "Epoch 8/20\n",
      "267/267 [==============================] - 22s 84ms/step - loss: 0.7595 - accuracy: 0.7390 - val_loss: 0.7484 - val_accuracy: 0.7418\n",
      "Epoch 9/20\n",
      "267/267 [==============================] - 22s 84ms/step - loss: 0.7170 - accuracy: 0.7553 - val_loss: 0.6626 - val_accuracy: 0.7706\n",
      "Epoch 10/20\n",
      "267/267 [==============================] - 23s 85ms/step - loss: 0.6772 - accuracy: 0.7697 - val_loss: 0.6546 - val_accuracy: 0.7723\n",
      "Epoch 11/20\n",
      "267/267 [==============================] - 23s 85ms/step - loss: 0.6467 - accuracy: 0.7786 - val_loss: 0.6466 - val_accuracy: 0.7750\n",
      "Epoch 12/20\n",
      "267/267 [==============================] - 23s 85ms/step - loss: 0.6102 - accuracy: 0.7921 - val_loss: 0.6315 - val_accuracy: 0.7798\n",
      "Epoch 13/20\n",
      "267/267 [==============================] - 22s 84ms/step - loss: 0.5811 - accuracy: 0.8003 - val_loss: 0.6151 - val_accuracy: 0.7890\n",
      "Epoch 14/20\n",
      "267/267 [==============================] - 22s 84ms/step - loss: 0.5578 - accuracy: 0.8095 - val_loss: 0.6007 - val_accuracy: 0.7928\n",
      "Epoch 15/20\n",
      "267/267 [==============================] - 23s 85ms/step - loss: 0.5364 - accuracy: 0.8158 - val_loss: 0.5875 - val_accuracy: 0.7981\n",
      "Epoch 16/20\n",
      "267/267 [==============================] - 22s 83ms/step - loss: 0.5127 - accuracy: 0.8237 - val_loss: 0.5927 - val_accuracy: 0.7951\n",
      "Epoch 17/20\n",
      "267/267 [==============================] - 22s 84ms/step - loss: 0.4936 - accuracy: 0.8311 - val_loss: 0.5964 - val_accuracy: 0.7954\n",
      "Epoch 18/20\n",
      "267/267 [==============================] - 22s 84ms/step - loss: 0.4741 - accuracy: 0.8384 - val_loss: 0.5652 - val_accuracy: 0.8052\n",
      "Epoch 19/20\n",
      "267/267 [==============================] - 23s 85ms/step - loss: 0.4583 - accuracy: 0.8434 - val_loss: 0.5729 - val_accuracy: 0.8051\n",
      "Epoch 20/20\n",
      "267/267 [==============================] - 23s 85ms/step - loss: 0.4360 - accuracy: 0.8511 - val_loss: 0.5468 - val_accuracy: 0.8155\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_43 (Conv2D)          (None, 32, 32, 256)       7168      \n",
      "                                                                 \n",
      " batch_normalization_23 (Ba  (None, 32, 32, 256)       1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_44 (Conv2D)          (None, 32, 32, 128)       295040    \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPooli  (None, 16, 16, 128)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_45 (Conv2D)          (None, 16, 16, 64)        204864    \n",
      "                                                                 \n",
      " batch_normalization_24 (Ba  (None, 16, 16, 64)        256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_46 (Conv2D)          (None, 16, 16, 64)        102464    \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPooli  (None, 8, 8, 64)          0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_47 (Conv2D)          (None, 2, 2, 32)          100384    \n",
      "                                                                 \n",
      " batch_normalization_25 (Ba  (None, 2, 2, 32)          128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 746922 (2.85 MB)\n",
      "Trainable params: 746218 (2.85 MB)\n",
      "Non-trainable params: 704 (2.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a Sequential model for the CNN architecture\n",
    "model_cifar = Sequential([\n",
    "    # Convolutional Layer 1\n",
    "    Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    # Convolutional Layer 2\n",
    "    Conv2D(filters=128, kernel_size=(3, 3), strides=1, padding='same', activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    # Convolutional Layer 3\n",
    "    Conv2D(filters=64, kernel_size=(5, 5), strides=1, padding='same', activation='sigmoid'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    # Convolutional Layer 4\n",
    "    Conv2D(filters=64, kernel_size=(5, 5), strides=1, padding='same', activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.35),\n",
    "\n",
    "    # Convolutional Layer 5\n",
    "    Conv2D(filters=32, kernel_size=(7, 7), strides=1, padding='valid', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    # Flatten Layer to convert the 2D feature maps to a 1D vector\n",
    "    Flatten(),\n",
    "\n",
    "    # Fully Connected Layer 1\n",
    "    Dense(256, activation='sigmoid'),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    # Output Layer with 10 units and softmax activation for classification\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_cifar.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define an early stopping callback\n",
    "callback = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# Train the model using the provided data and labels\n",
    "model_history_cifar = model_cifar.fit(train_data_cifar, train_labels_cifar, batch_size=150,\n",
    "                                      epochs=20,\n",
    "                                      validation_data=(val_data_cifar, val_labels_cifar),\n",
    "                                      callbacks=[callback],\n",
    "                                      verbose=1)\n",
    "\n",
    "# Display a summary of the model's architecture\n",
    "model_cifar.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rSc6FNRuRnTb",
    "outputId": "4f2ad368-863f-4445-c325-c1824b7d79e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5745 - accuracy: 0.8078\n",
      "Test Accuracy: 0.8077999949455261\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss_cifar, test_acc_cifar = model_cifar.evaluate(test_data_cifar, test_labels_cifar)\n",
    "print('Test Accuracy:', test_acc_cifar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yKuywj3NV3B"
   },
   "source": [
    "## Exercise 2: Describe What you did\n",
    "\n",
    "All the work you did leading up to your final model should be summarized in this section. This should be a logical and well-organized summary of the various experiments that were tried in **Lab 3 Part 2 - Task 2:Exercise 1**, and should be captured in **table format**. Upon reading this section I should understand what you tried, the reasoning behind trying it, any quantitative values that correspond to what you tried, and the results.\n",
    "\n",
    "See [this guide](https://www.datacamp.com/community/tutorials/markdown-in-jupyter-notebook) for how to format markdown cells in Jupyter notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFN1cTRoOmyt"
   },
   "source": [
    "# Summary\n",
    "\n",
    "- Our main objective was to achieve at least 80% accuracy on validation data, And it was to be done under the thresold of 20 epochs. To achieve this we have tried and trained models using different different achitectural designs. Here, are our observations based on those tries.\n",
    "\n",
    "- We started with our base model as our model from LAB 3 part 1. This model had 4 convolution layers in it with 32, 32, 64 and 128 filters respectively. Then, after flattening the data we added two dense layers one of which was output layer. We used the ReLU activation function for all layers except the output layer, which used softmax. The model used the 'rmsprop' optimizer and 'categorical_crossentropy' as the loss function. We achieved 63% of validation activity using our base model.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "| Experiment | Changes | Reasoning behind the changes | Validation Accuracy |\n",
    "| --- | --- | --- | --- |\n",
    "| 1 | Increased number of filters | This helps model to understand complex patterns | 69.3%  |\n",
    "| 2 | Added fifth convolution layer | Improves feature extraction and model complexity | 72.2% |\n",
    "| 3 | Added Batch Normalization | Ensure stable convergence and speeds up the learning process of model | 73.4% |\n",
    "| 4 | Added MaxPooling2D layers | Reduce overfitting and capture Distinctive and dominant features | 74% |\n",
    "| 5 | Added Dropout layers | Futher reduces overfitting and improves genralization | 74.6% |\n",
    "| 6 | Tried Different dropout rates | Model's training accuracy was constantly increasing but validation accuracy was stuck as 74% | 75% |\n",
    "| 7 | Changed some padings to ***valid*** | same padding preserves the spatial imformation and valid padding helps in lowering the computational costs | 76% |\n",
    "| 8 | Implemented EarlyStopping | Prevent overfitting and terminate training at the optimal point if it reaches there before reaching the last epoch | 76.1% |\n",
    "| 9 | Changed optimizer to Adam with 0.0001 learning rate | Adam has benifits of both RMSprop and momentum methods (minimizes cost) also faster trainnig. We also tried RMSprop, SGD. | 77.9% |\n",
    "| 10 | Tried Different number of epochs and batch sizes | Helps in balancing speed, generalization, and overall model performance | 79.1% |\n",
    "| 11 | Changed some of the activation functions to sigmoid from relu | Sigmoid helps in image classification problems | 80%|\n",
    "| 12 | Changed shape of filters (kernal_size) | This helps in identifying different sized patterns, small filter identifies small patterns and big identifies big patterns| 81.5% |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    ">\n",
    "- Our model achived 80.78% accuracy on test data as well, this shows us that our model has understood the patterns and working fine with unseen data.\n",
    "\n",
    "- As the above table suggests, there are many factors which helps in improving the model. The validation accuracies are not exact values but these values suggests the impact of the change which were made.\n",
    "\n",
    "- The ***change in filter shape*** and **adding sigmoid activation** function was the major breakthrough for us as we were stuck on 78-79% validation accuracy. But after changing those two we were constantly getting validation accuracy above 80%.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
