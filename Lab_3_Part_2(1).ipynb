{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIT-maBq2bbZ"
   },
   "source": [
    "# Lab 3 Part 2 - Task 1: Parameters in CNN (5 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWo32rVm8TGH"
   },
   "source": [
    "- For the model we have created in **Lab 3 Part 1 Exercise**: Early Stopping with Callbacks, calculate the number of parameters by hand for each layer and compare to the output of model.summary() and print the model summary.\n",
    "- Then print the model summary of **Exercise 7 in Lab 1**\n",
    "- Now compare the Model you created in **Exercise 7 in Lab 1**,\n",
    "  - Compare the Parameters of the models\n",
    "\n",
    "  - Compare Model Performance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed to get same results everytime.\n",
    "tf.random.set_seed(101)\n",
    "np.random.seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the first 10,000 samples of our training data as our validation set\n",
    "val_data = train_data[:10000]\n",
    "val_labels = train_labels[:10000]\n",
    "\n",
    "# Use the remainder of the original training data for actual training\n",
    "partial_train_data = train_data[10000:]\n",
    "partial_train_labels = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the pixel values so they lie in the range of 0-1\n",
    "partial_train_data = partial_train_data / 255.\n",
    "val_data = val_data / 255.\n",
    "test_data = test_data /255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding the dimensions of partial_train_data, val_data and test_data to make it 4D.\n",
    "partial_train_data = np.expand_dims(partial_train_data, axis=3)\n",
    "val_data = np.expand_dims(val_data, axis=3)\n",
    "test_data = np.expand_dims(test_data, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 28, 28, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the labels into one hot encoded format \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "partial_train_labels = to_categorical(partial_train_labels)\n",
    "val_labels = to_categorical(val_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "196/196 [==============================] - 49s 242ms/step - loss: 0.2659 - accuracy: 0.9198 - val_loss: 0.0710 - val_accuracy: 0.9792\n",
      "Epoch 2/15\n",
      "196/196 [==============================] - 47s 241ms/step - loss: 0.0494 - accuracy: 0.9849 - val_loss: 0.0487 - val_accuracy: 0.9849\n",
      "Epoch 3/15\n",
      "196/196 [==============================] - 48s 245ms/step - loss: 0.0289 - accuracy: 0.9907 - val_loss: 0.0423 - val_accuracy: 0.9882\n",
      "Epoch 4/15\n",
      "196/196 [==============================] - 49s 251ms/step - loss: 0.0191 - accuracy: 0.9941 - val_loss: 0.0760 - val_accuracy: 0.9797\n",
      "Epoch 5/15\n",
      "196/196 [==============================] - 49s 251ms/step - loss: 0.0119 - accuracy: 0.9963 - val_loss: 0.0520 - val_accuracy: 0.9869\n",
      "Epoch 6/15\n",
      "196/196 [==============================] - 49s 251ms/step - loss: 0.0096 - accuracy: 0.9967 - val_loss: 0.0380 - val_accuracy: 0.9905\n",
      "Epoch 7/15\n",
      "196/196 [==============================] - 47s 239ms/step - loss: 0.0067 - accuracy: 0.9978 - val_loss: 0.0473 - val_accuracy: 0.9884\n",
      "Epoch 8/15\n",
      "196/196 [==============================] - 47s 242ms/step - loss: 0.0052 - accuracy: 0.9983 - val_loss: 0.0557 - val_accuracy: 0.9882\n",
      "Epoch 9/15\n",
      "196/196 [==============================] - 47s 241ms/step - loss: 0.0044 - accuracy: 0.9987 - val_loss: 0.0609 - val_accuracy: 0.9876\n",
      "Epoch 10/15\n",
      "196/196 [==============================] - 47s 242ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.0681 - val_accuracy: 0.9888\n",
      "Epoch 11/15\n",
      "196/196 [==============================] - 49s 248ms/step - loss: 0.0021 - accuracy: 0.9993 - val_loss: 0.0708 - val_accuracy: 0.9893\n",
      "Epoch 12/15\n",
      "196/196 [==============================] - 49s 248ms/step - loss: 0.0024 - accuracy: 0.9991 - val_loss: 0.0695 - val_accuracy: 0.9871\n",
      "Epoch 13/15\n",
      "196/196 [==============================] - 49s 250ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.0953 - val_accuracy: 0.9869\n",
      "Epoch 14/15\n",
      "196/196 [==============================] - 49s 249ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0796 - val_accuracy: 0.9893\n",
      "Epoch 15/15\n",
      "196/196 [==============================] - 49s 251ms/step - loss: 0.0015 - accuracy: 0.9994 - val_loss: 0.0796 - val_accuracy: 0.9903\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 13, 13, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 13, 13, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 11, 11, 128)       73856     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 15488)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               1982592   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2085802 (7.96 MB)\n",
      "Trainable params: 2085802 (7.96 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define a Sequential model for the CNN architecture.\n",
    "model = Sequential([\n",
    "    # Convolutional Layer 1\n",
    "    Conv2D(filters=32,\n",
    "           kernel_size=(3, 3),\n",
    "           strides=1,\n",
    "           padding='same',\n",
    "           activation='relu',\n",
    "           input_shape=(28, 28, 1)),\n",
    "\n",
    "    # Convolutional Layer 2\n",
    "    Conv2D(filters=32,\n",
    "           kernel_size=(3, 3),\n",
    "           strides=2,\n",
    "           padding='valid',\n",
    "           activation='relu'),\n",
    "\n",
    "    # Convolutional Layer 3\n",
    "    Conv2D(filters=64,\n",
    "           kernel_size=(3, 3),\n",
    "           strides=1,\n",
    "           padding='same',\n",
    "           activation='relu'),\n",
    "\n",
    "    # Convolutional Layer 4\n",
    "    Conv2D(filters=128,\n",
    "           kernel_size=(3, 3),\n",
    "           strides=1,\n",
    "           padding='valid',\n",
    "           activation='relu'),\n",
    "\n",
    "    # Flatten Layer to convert the 2D feature maps to a 1D vector\n",
    "    Flatten(),\n",
    "\n",
    "    # Fully Connected Layer 1\n",
    "    Dense(128, activation='relu'),\n",
    "\n",
    "    # Output Layer with 10 units and softmax activation for classification\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model, specifying the optimizer, loss function, and metrics.\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Define an early stopping callback to monitor the training process.\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)\n",
    "\n",
    "# Train the model using the provided data and labels.\n",
    "model_history = model.fit(partial_train_data,\n",
    "                          partial_train_labels,\n",
    "                          epochs=15,\n",
    "                          batch_size=256,\n",
    "                          callbacks=[callback],\n",
    "                          validation_data=(val_data, val_labels),\n",
    "                          verbose=1)\n",
    "\n",
    "# Display a summary of the model's architecture.\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_fnn = partial_train_data.astype('float32') / 255\n",
    "train_images_fnn = partial_train_data.reshape((50000, 28 * 28))\n",
    "\n",
    "test_images_fnn = val_data.astype('float32') / 255\n",
    "test_images_fnn = val_data.reshape((10000, 28 * 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "PWci6yrl2Zlg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_35 (Dense)            (None, 500)               392500    \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 100)               50100     \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 443610 (1.69 MB)\n",
      "Trainable params: 443610 (1.69 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1471/1471 [==============================] - 14s 9ms/step - loss: 0.2176 - accuracy: 0.9338\n",
      "Epoch 2/5\n",
      "1471/1471 [==============================] - 13s 9ms/step - loss: 0.0917 - accuracy: 0.9714\n",
      "Epoch 3/5\n",
      "1471/1471 [==============================] - 13s 9ms/step - loss: 0.0637 - accuracy: 0.9801\n",
      "Epoch 4/5\n",
      "1471/1471 [==============================] - 13s 9ms/step - loss: 0.0475 - accuracy: 0.9849\n",
      "Epoch 5/5\n",
      "1471/1471 [==============================] - 13s 9ms/step - loss: 0.0344 - accuracy: 0.9892\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0864 - accuracy: 0.9768\n",
      "test_acc: 0.9768000245094299\n"
     ]
    }
   ],
   "source": [
    "sequential_7 = Sequential()\n",
    "sequential_7.add(Dense(500, activation='relu', input_shape=(28 * 28,)))\n",
    "sequential_7.add(Dense(100, activation='tanh', input_shape=(500,), kernel_initializer=GlorotUniform()))\n",
    "sequential_7.add(Dropout(0.25))\n",
    "sequential_7.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sequential_7.summary()\n",
    "\n",
    "sequential_7.compile(optimizer=Adam(learning_rate = 0.001),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "sequential_7.fit(train_images_fnn, partial_train_labels, epochs=5, batch_size=34, verbose=1)\n",
    "\n",
    "test_loss, test_acc = sequential_7.evaluate(test_images_fnn, val_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "313/313 [==============================] - 1s 4ms/step - loss: 0.0899 - accuracy: 0.9766\n",
    "test_acc: 0.9765999913215637"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\solan\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\solan\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\solan\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\solan\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1081, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\solan\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\solan\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\solan\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\solan\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\solan\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py\", line 2122, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\solan\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py\", line 5560, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 10) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13420\\2861033530.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m model4_history = model4.fit(train_images,\n\u001b[0m\u001b[0;32m     36\u001b[0m                     \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\solan\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\solan\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\solan\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\solan\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1081, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\solan\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\solan\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\solan\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\solan\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\solan\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py\", line 2122, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\solan\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py\", line 5560, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 10) are incompatible\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential([\n",
    "    Conv2D(filters=32,\n",
    "           kernel_size=(3, 3),\n",
    "           strides=1,\n",
    "           padding='same',\n",
    "           activation='relu',\n",
    "           input_shape=(28, 28, 1)),\n",
    "    Conv2D(filters=32,\n",
    "           kernel_size=(3, 3),\n",
    "           strides=2,\n",
    "           padding='valid',\n",
    "           activation='relu'),\n",
    "    Conv2D(filters=64,\n",
    "           kernel_size=(3, 3),\n",
    "           strides=1,\n",
    "           padding='same',\n",
    "          activation='relu'),\n",
    "    Conv2D(filters=128,\n",
    "           kernel_size=(3, 3),\n",
    "           strides=1,\n",
    "           padding='valid',\n",
    "           activation='relu'),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model4.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)\n",
    "\n",
    "model4_history = model4.fit(train_images,\n",
    "                    train_labels,\n",
    "                    epochs=15,\n",
    "                    batch_size=256,\n",
    "                    callbacks=[callback],\n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maXaHAcw2TsB"
   },
   "source": [
    "# Lab 3 Part 2 - Task 2: CIFAR-10 Challenge (10 Marks)\n",
    "\n",
    "In this lab you will experiment with whatever ConvNet architecture/design you'd like on [CIFAR-10 image dataset](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "\n",
    "## Exercise  1: Creating the network\n",
    "\n",
    "**Goal:** After training, your model should achieve **at least 80%** accuracy on a **validation** set within 20 epochs. (Or as close as possible as long as there is demonstrated effort to achieve this goal.)\n",
    "\n",
    "**Data split** The training set should consist of 40000 images, the validation set should consist of 10000 images, and the test set should consist of the remaining 10000 images. **Please use the Keras `load_data()` function to import the data set.**\n",
    "\n",
    "\n",
    "### Some things you can try:\n",
    "- Different number/type of layers\n",
    "- Different filter sizes\n",
    "- Adjust the number of filters used in any given layer\n",
    "- Try various pooling strategies\n",
    "- Consider using batch normalization\n",
    "- Check if adding regularization helps\n",
    "- Consider alternative optimizers\n",
    "- Try different activation functions\n",
    "\n",
    "\n",
    "### Tips for training\n",
    "When building/tuning your model, keep in mind the following points:\n",
    "\n",
    "- This is experimental, so be driven by results achieved on the validation set as opposed to what you have heard/read works well or doesn't\n",
    "- If the hyperparameters are working well, you should see improvement in the loss/accuracy within approximately one epoch\n",
    "- For hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all\n",
    "- Once you have found some sets of hyperparameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
    "- Prefer random search to grid search for hyperparameters\n",
    "- You should use the validation set for hyperparameter search and for evaluating different architectures\n",
    "- The test set should only be used at the very end to evaluate your final model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tIOVFQ0m2TsG"
   },
   "outputs": [],
   "source": [
    "## Exercise 2: Describe What you did\n",
    "\n",
    "All the work you did leading up to your final model should be summarized in this section. This should be a logical and well-organized summary of the various experiments that were tried in **Lab 3 Part 2 - Task 2:Exercise 1**, and should be captured in **table format**. Upon reading this section I should understand what you tried, the reasoning behind trying it, any quantitative values that correspond to what you tried, and the results.\n",
    "\n",
    "See [this guide](https://www.datacamp.com/community/tutorials/markdown-in-jupyter-notebook) for how to format markdown cells in Jupyter notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64BbG8Y62TsH"
   },
   "source": [
    "## Exercise 2: Describe What you did\n",
    "\n",
    "All the work you did leading up to your final model should be summarized in this section. This should be a logical and well-organized summary of the various experiments that were tried in **Lab 3 Part 2 - Task 2:Exercise 1**, and should be captured in **table format**. Upon reading this section I should understand what you tried, the reasoning behind trying it, any quantitative values that correspond to what you tried, and the results.\n",
    "\n",
    "See [this guide](https://www.datacamp.com/community/tutorials/markdown-in-jupyter-notebook) for how to format markdown cells in Jupyter notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sa8p0Nkj2TsI"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
